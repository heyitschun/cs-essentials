Time complexity is the computational complexity that describes the amount of time it takes to run an algorithm. The exact amount of time it takes the computer to run an operation, of course, depends on the computer's power. In computer science, the runtime of an operation refers to the number of instructions performed during the operation. 

Runtimes are described with the big O notation and is not limited to time. Common runtimes are `O(n)`, `O(log n)`, `O(n log n)` and `O(n**2)`. But this list is not exhaustive and a runtime can have multiple variables.

Big O is about the expression of runtime *scaling*, more so than anything. Highly precise notation is actually not the point of using big O, which is why constants and non-dominant terms can be dropped in big O notation. It's all about scaling.

Big O notation can also be used to describe the amount of memory allocation the algorithm requires.

# Read More

- [Open Data Structures](https://opendatastructures.org/ods-python/1_3_Mathematical_Background.html#950)
- [Wikipedia](https://en.wikipedia.org/wiki/Time_complexity)
